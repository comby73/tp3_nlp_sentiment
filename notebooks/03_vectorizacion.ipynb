{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbaa1460",
   "metadata": {},
   "source": [
    "## 1. Configuraci√≥n e Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c461d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Librer√≠as y m√≥dulos cargados\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Agregar src al path\n",
    "project_dir = Path.cwd().parent\n",
    "\n",
    "# Agregar src al path expl√≠citamente para imports internos\n",
    "src_dir = project_dir / 'src'\n",
    "if str(src_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(src_dir))\n",
    "if str(project_dir) not in sys.path:\n",
    "    sys.path.append(str(project_dir))\n",
    "\n",
    "from src.data_loading import load_processed_data, save_object\n",
    "from src.features import create_tfidf_features, get_top_tfidf_words\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import os\n",
    "\n",
    "print('‚úì Librer√≠as y m√≥dulos cargados')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ef27eb",
   "metadata": {},
   "source": [
    "## 2. Cargar Datos Procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0087d35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Datos procesados cargados: 1596781 filas\n",
      "‚úì Datos procesados cargados: 359 filas\n",
      "Train: (1596781, 11)\n",
      "Test: (359, 11)\n",
      "\n",
      "Columnas: ['polarity', 'text', 'text_clean', 'length', 'num_words', 'num_hashtags', 'num_mentions', 'num_urls', 'num_uppercase', 'pct_uppercase', 'num_intensified']\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos procesados usando src\n",
    "df_train = load_processed_data('train_processed.csv')\n",
    "df_test = load_processed_data('test_processed.csv')\n",
    "\n",
    "print(f\"Train: {df_train.shape}\")\n",
    "print(f\"Test: {df_test.shape}\")\n",
    "print(f\"\\nColumnas: {list(df_train.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb99bc43",
   "metadata": {},
   "source": [
    "## 3. Configurar Stopwords Personalizadas\n",
    "\n",
    "**Decisi√≥n del EDA:** Mantener palabras que aportan sentimiento como \"not\", \"no\", \"very\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "400da738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords originales: 318\n",
      "Stopwords personalizadas: 291\n",
      "Palabras de sentimiento conservadas: 35\n",
      "\n",
      "Ejemplos conservados: ['neither', 'best', 'really', 'some', 'every', 'nowhere', 'no', 'nor', 'though', 'however']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Palabras que S√ç aportan sentimiento (las sacamos de stopwords)\n",
    "sentiment_words = {\n",
    "    'not', 'no', 'nor', 'neither', 'never', 'none', 'nobody', 'nothing', 'nowhere',\n",
    "    'very', 'really', 'so', 'too', 'quite', 'rather',\n",
    "    'but', 'however', 'although', 'though',\n",
    "    'all', 'every', 'any', 'some', 'most',\n",
    "    'much', 'many', 'more', 'most', 'less', 'least',\n",
    "    'good', 'bad', 'best', 'worst', 'better', 'worse'\n",
    "}\n",
    "\n",
    "# Stopwords finales = stopwords default - palabras de sentimiento\n",
    "custom_stopwords = ENGLISH_STOP_WORDS - sentiment_words\n",
    "\n",
    "print(f\"Stopwords originales: {len(ENGLISH_STOP_WORDS)}\")\n",
    "print(f\"Stopwords personalizadas: {len(custom_stopwords)}\")\n",
    "print(f\"Palabras de sentimiento conservadas: {len(sentiment_words)}\")\n",
    "print(f\"\\nEjemplos conservados: {list(sentiment_words)[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b86d990",
   "metadata": {},
   "source": [
    "## 4. Crear Vectorizador TF-IDF con Bigramas\n",
    "\n",
    "**Configuraci√≥n:**\n",
    "- **ngram_range=(1,2)**: palabras individuales + bigramas\n",
    "- **max_features=5000**: limitar vocabulario\n",
    "- **min_df=5**: palabra debe aparecer al menos 5 veces\n",
    "- **max_df=0.8**: palabra no puede estar en m√°s del 80% de tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7bcc819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La configuraci√≥n del vectorizador se pasa directamente a la funci√≥n create_tfidf_features de src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e11119",
   "metadata": {},
   "source": [
    "## 5. Entrenar TF-IDF en TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e882f67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape Train: (1596781, 10000)\n",
      "Shape Test: (359, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Crear features TF-IDF usando src\n",
    "# Esta funci√≥n entrena en train y transforma test autom√°ticamente\n",
    "X_train_tfidf, X_test_tfidf, tfidf_vectorizer = create_tfidf_features(\n",
    "    train_texts=df_train['text_clean'].fillna(''),\n",
    "    test_texts=df_test['text_clean'].fillna(''),\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words=list(custom_stopwords),\n",
    "    min_df=5,\n",
    "    max_df=0.8\n",
    ")\n",
    "\n",
    "print(f\"\\nShape Train: {X_train_tfidf.shape}\")\n",
    "print(f\"Shape Test: {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1cf437",
   "metadata": {},
   "source": [
    "## 6. Transformar TEST\n",
    "\n",
    "**Nota**: La transformaci√≥n de TEST ya se realiz√≥ autom√°ticamente en la celda anterior \n",
    "mediante `create_tfidf_features()`, que entrena en TRAIN y transforma TEST sin data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293607ec",
   "metadata": {},
   "source": [
    "## 7. Inspeccionar Vocabulario Aprendido\n",
    "\n",
    "Verificamos que incluya bigramas importantes como \"not good\", \"very happy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f44df249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VOCABULARIO ===\n",
      "\n",
      "Total t√©rminos: 10000\n",
      "  - Unigramas: 6121\n",
      "  - Bigramas: 3879\n",
      "\n",
      "üìù Ejemplos de bigramas importantes:\n",
      "['absolutely love', 'absolutely nothing', 'actually really', 'all annoying', 'all know', 'all not', 'all really', 'all so', 'all songs', 'all too', 'all very', 'amazing so', 'amp no', 'amp not', 'amp so', 'amp some', 'annoying help', 'anytime soon', 'ate some', 'ate too']\n"
     ]
    }
   ],
   "source": [
    "# Obtener t√©rminos y sus √≠ndices\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Separar unigramas y bigramas\n",
    "unigramas = [term for term in feature_names if ' ' not in term]\n",
    "bigramas = [term for term in feature_names if ' ' in term]\n",
    "\n",
    "print(f\"=== VOCABULARIO ===\")\n",
    "print(f\"\\nTotal t√©rminos: {len(feature_names)}\")\n",
    "print(f\"  - Unigramas: {len(unigramas)}\")\n",
    "print(f\"  - Bigramas: {len(bigramas)}\")\n",
    "\n",
    "print(f\"\\nüìù Ejemplos de bigramas importantes:\")\n",
    "sentiment_bigrams = [b for b in bigramas if any(word in b for word in ['not', 'no', 'very', 'really', 'so', 'too'])]\n",
    "print(sentiment_bigrams[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "critical_bigrams",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Verificando bigramas cr√≠ticos:\n",
      "  ‚úì 'not good' est√° en el vocabulario\n",
      "  ‚úì 'not bad' est√° en el vocabulario\n",
      "  ‚úì 'very happy' est√° en el vocabulario\n",
      "  ‚úì 'very sad' est√° en el vocabulario\n",
      "  ‚úì 'dont like' est√° en el vocabulario\n",
      "  ‚úó 'cant wait' NO est√° en el vocabulario\n",
      "  ‚úì 'so sad' est√° en el vocabulario\n",
      "  ‚úì 'so happy' est√° en el vocabulario\n"
     ]
    }
   ],
   "source": [
    "# Verificar bigramas cr√≠ticos de sentimiento\n",
    "critical_bigrams = ['not good', 'not bad', 'very happy', 'very sad', \n",
    "                    'dont like', 'cant wait', 'so sad', 'so happy']\n",
    "\n",
    "print(\"\\nüîç Verificando bigramas cr√≠ticos:\")\n",
    "for bigram in critical_bigrams:\n",
    "    if bigram in tfidf_vectorizer.vocabulary_:\n",
    "        print(f\"  ‚úì '{bigram}' est√° en el vocabulario\")\n",
    "    else:\n",
    "        print(f\"  ‚úó '{bigram}' NO est√° en el vocabulario\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec_7_1_md",
   "metadata": {},
   "source": [
    "## 7.1 Top Features por Clase usando TF-IDF\n",
    "\n",
    "Analizamos qu√© t√©rminos son m√°s relevantes para cada clase (Positivo vs Negativo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sec_7_1_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analizando top features por clase...\n",
      "üü¢ TOP 10 T√âRMINOS EN TWEETS POSITIVOS:\n",
      "  ‚Ä¢ love\n",
      "  ‚Ä¢ im\n",
      "  ‚Ä¢ thanks\n",
      "  ‚Ä¢ got\n",
      "  ‚Ä¢ too\n",
      "  ‚Ä¢ just\n",
      "  ‚Ä¢ so\n",
      "  ‚Ä¢ time\n",
      "  ‚Ä¢ like\n",
      "  ‚Ä¢ day\n",
      "\n",
      "üî¥ TOP 10 T√âRMINOS EN TWEETS NEGATIVOS:\n",
      "  ‚Ä¢ got\n",
      "  ‚Ä¢ day\n",
      "  ‚Ä¢ today\n",
      "  ‚Ä¢ like\n",
      "  ‚Ä¢ no\n",
      "  ‚Ä¢ not\n",
      "  ‚Ä¢ all\n",
      "  ‚Ä¢ im\n",
      "  ‚Ä¢ just\n",
      "  ‚Ä¢ but\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Calcular promedios de TF-IDF por clase\n",
    "positive_tweets = df_train[df_train['polarity'] == 4]['text_clean'].fillna('')\n",
    "negative_tweets = df_train[df_train['polarity'] == 0]['text_clean'].fillna('')\n",
    "\n",
    "# Entrenar vectorizadores separados para an√°lisis exploratorio\n",
    "print(\"Analizando top features por clase...\")\n",
    "vec_pos = TfidfVectorizer(max_features=20, ngram_range=(1,2), stop_words=list(custom_stopwords))\n",
    "vec_neg = TfidfVectorizer(max_features=20, ngram_range=(1,2), stop_words=list(custom_stopwords))\n",
    "\n",
    "X_pos = vec_pos.fit_transform(positive_tweets)\n",
    "X_neg = vec_neg.fit_transform(negative_tweets)\n",
    "\n",
    "print(\"üü¢ TOP 10 T√âRMINOS EN TWEETS POSITIVOS:\")\n",
    "for term in list(vec_pos.vocabulary_.keys())[:10]:\n",
    "    print(f\"  ‚Ä¢ {term}\")\n",
    "\n",
    "print(\"\\nüî¥ TOP 10 T√âRMINOS EN TWEETS NEGATIVOS:\")\n",
    "for term in list(vec_neg.vocabulary_.keys())[:10]:\n",
    "    print(f\"  ‚Ä¢ {term}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bdb3702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîù Top 20 t√©rminos m√°s importantes (TF-IDF promedio):\n",
      "      word  tfidf_score\n",
      "0       im     0.018156\n",
      "1       so     0.015834\n",
      "2     just     0.014049\n",
      "3      but     0.013185\n",
      "4      not     0.012851\n",
      "5     good     0.011934\n",
      "6      day     0.011151\n",
      "7       no     0.010250\n",
      "8      all     0.010094\n",
      "9     like     0.010013\n",
      "10    love     0.009807\n",
      "11    work     0.009619\n",
      "12     too     0.009448\n",
      "13   going     0.009181\n",
      "14    dont     0.009137\n",
      "15   today     0.009093\n",
      "16     got     0.008427\n",
      "17     lol     0.008413\n",
      "18  thanks     0.008405\n",
      "19    time     0.008002\n"
     ]
    }
   ],
   "source": [
    "# Top t√©rminos usando src\n",
    "top_words_df = get_top_tfidf_words(tfidf_vectorizer, X_train_tfidf, n_top=20)\n",
    "\n",
    "print(\"\\nüîù Top 20 t√©rminos m√°s importantes (TF-IDF promedio):\")\n",
    "print(top_words_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b21aa",
   "metadata": {},
   "source": [
    "## 8. Preparar Features Num√©ricas\n",
    "\n",
    "Las 7 features extra√≠das en el preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf21508f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features num√©ricas train: (1596781, 7)\n",
      "Features num√©ricas test: (359, 7)\n",
      "\n",
      "üìä Estad√≠sticas features num√©ricas (TRAIN):\n",
      "           length   num_words  num_hashtags  num_mentions    num_urls  \\\n",
      "count  1596781.00  1596781.00    1596781.00    1596781.00  1596781.00   \n",
      "mean        74.20       13.20          0.03          0.49        0.05   \n",
      "std         36.38        6.94          0.22          0.59        0.23   \n",
      "min          6.00        1.00          0.00          0.00        0.00   \n",
      "25%         44.00        7.00          0.00          0.00        0.00   \n",
      "50%         69.00       12.00          0.00          0.00        0.00   \n",
      "75%        104.00       19.00          0.00          1.00        0.00   \n",
      "max        374.00       64.00         24.00         12.00        5.00   \n",
      "\n",
      "       num_uppercase  pct_uppercase  \n",
      "count     1596781.00     1596781.00  \n",
      "mean            3.26           6.22  \n",
      "std             5.21           9.77  \n",
      "min             0.00           0.00  \n",
      "25%             1.00           1.16  \n",
      "50%             2.00           4.08  \n",
      "75%             4.00           7.61  \n",
      "max           131.00         100.00  \n"
     ]
    }
   ],
   "source": [
    "# Columnas de features num√©ricas\n",
    "numeric_features = ['length', 'num_words', 'num_hashtags', 'num_mentions', \n",
    "                   'num_urls', 'num_uppercase', 'pct_uppercase']\n",
    "\n",
    "# Extraer features num√©ricas\n",
    "X_train_numeric = df_train[numeric_features].values\n",
    "X_test_numeric = df_test[numeric_features].values\n",
    "\n",
    "print(f\"Features num√©ricas train: {X_train_numeric.shape}\")\n",
    "print(f\"Features num√©ricas test: {X_test_numeric.shape}\")\n",
    "\n",
    "print(f\"\\nüìä Estad√≠sticas features num√©ricas (TRAIN):\")\n",
    "print(df_train[numeric_features].describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3488837b",
   "metadata": {},
   "source": [
    "## 9. Combinar TF-IDF + Features Num√©ricas\n",
    "\n",
    "Matriz final = [TF-IDF texto] + [7 features num√©ricas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1de69f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinando features...\n",
      "\n",
      "‚úì Features combinadas\n",
      "\n",
      "TRAIN:\n",
      "  - TF-IDF: (1596781, 10000)\n",
      "  - Num√©ricas: (1596781, 7)\n",
      "  - FINAL: (1596781, 10007)\n",
      "\n",
      "TEST:\n",
      "  - TF-IDF: (359, 10000)\n",
      "  - Num√©ricas: (359, 7)\n",
      "  - FINAL: (359, 10007)\n"
     ]
    }
   ],
   "source": [
    "print(\"Combinando features...\\n\")\n",
    "\n",
    "# Convertir features num√©ricas a sparse matrix\n",
    "X_train_numeric_sparse = csr_matrix(X_train_numeric)\n",
    "X_test_numeric_sparse = csr_matrix(X_test_numeric)\n",
    "\n",
    "# Concatenar horizontalmente (TF-IDF + numeric)\n",
    "X_train_final = hstack([X_train_tfidf, X_train_numeric_sparse])\n",
    "X_test_final = hstack([X_test_tfidf, X_test_numeric_sparse])\n",
    "\n",
    "print(\"‚úì Features combinadas\")\n",
    "print(f\"\\nTRAIN:\")\n",
    "print(f\"  - TF-IDF: {X_train_tfidf.shape}\")\n",
    "print(f\"  - Num√©ricas: {X_train_numeric.shape}\")\n",
    "print(f\"  - FINAL: {X_train_final.shape}\")\n",
    "\n",
    "print(f\"\\nTEST:\")\n",
    "print(f\"  - TF-IDF: {X_test_tfidf.shape}\")\n",
    "print(f\"  - Num√©ricas: {X_test_numeric.shape}\")\n",
    "print(f\"  - FINAL: {X_test_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "memory_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ USO DE MEMORIA:\n",
      "  X_train_final: 132.8 MB\n",
      "  X_test_final: 0.0 MB\n",
      "  Equivalente denso: 119.1 GB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(f\"\\nüíæ USO DE MEMORIA:\")\n",
    "print(f\"  X_train_final: {X_train_final.data.nbytes / 1024**2:.1f} MB\")\n",
    "print(f\"  X_test_final: {X_test_final.data.nbytes / 1024**2:.1f} MB\")\n",
    "print(f\"  Equivalente denso: {(X_train_final.shape[0] * X_train_final.shape[1] * 8) / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae929378",
   "metadata": {},
   "source": [
    "## 10. Preparar Etiquetas (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dd0f9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train: (1596781,)\n",
      "y_test: (359,)\n",
      "\n",
      "Balance TRAIN:\n",
      "  - Negativos (0): 798383\n",
      "  - Positivos (1): 798398\n",
      "\n",
      "Balance TEST:\n",
      "  - Negativos (0): 177\n",
      "  - Positivos (1): 182\n"
     ]
    }
   ],
   "source": [
    "# Convertir polaridad 0/4 a 0/1 para clasificaci√≥n binaria\n",
    "y_train = (df_train['polarity'].values == 4).astype(int)\n",
    "y_test = (df_test['polarity'].values == 4).astype(int)\n",
    "\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nBalance TRAIN:\")\n",
    "print(f\"  - Negativos (0): {(y_train == 0).sum()}\")\n",
    "print(f\"  - Positivos (1): {(y_train == 1).sum()}\")\n",
    "\n",
    "print(f\"\\nBalance TEST:\")\n",
    "print(f\"  - Negativos (0): {(y_test == 0).sum()}\")\n",
    "print(f\"  - Positivos (1): {(y_test == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82342ed1",
   "metadata": {},
   "source": [
    "## 11. Guardar Vectores y Objetos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4963592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Objeto guardado en: d:\\Diplomatura en ia\\trabajo practico 3 -Omar Gonzalez\\tp3_nlp_sentiment\\data\\processed\\..\\vectorized\\X_train.pkl\n",
      "‚úì Objeto guardado en: d:\\Diplomatura en ia\\trabajo practico 3 -Omar Gonzalez\\tp3_nlp_sentiment\\data\\processed\\..\\vectorized\\X_test.pkl\n",
      "‚úì Objeto guardado en: d:\\Diplomatura en ia\\trabajo practico 3 -Omar Gonzalez\\tp3_nlp_sentiment\\data\\processed\\..\\vectorized\\y_train.pkl\n",
      "‚úì Objeto guardado en: d:\\Diplomatura en ia\\trabajo practico 3 -Omar Gonzalez\\tp3_nlp_sentiment\\data\\processed\\..\\vectorized\\y_test.pkl\n",
      "‚úì Objeto guardado en: d:\\Diplomatura en ia\\trabajo practico 3 -Omar Gonzalez\\tp3_nlp_sentiment\\data\\processed\\..\\vectorized\\tfidf_vectorizer.pkl\n",
      "‚úì Datos vectorizados guardados\n"
     ]
    }
   ],
   "source": [
    "# Guardar objetos usando src\n",
    "import os\n",
    "os.makedirs('../data/vectorized', exist_ok=True)\n",
    "\n",
    "save_object(X_train_final, '../vectorized/X_train.pkl')\n",
    "save_object(X_test_final, '../vectorized/X_test.pkl')\n",
    "save_object(y_train, '../vectorized/y_train.pkl')\n",
    "save_object(y_test, '../vectorized/y_test.pkl')\n",
    "save_object(tfidf_vectorizer, '../vectorized/tfidf_vectorizer.pkl')\n",
    "\n",
    "print(\"‚úì Datos vectorizados guardados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "vocab_coverage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä COBERTURA DE VOCABULARIO:\n",
      "  T√©rminos √∫nicos en TEST: 1584\n",
      "  T√©rminos en vocabulario TF-IDF: 10000\n",
      "  T√©rminos en com√∫n: 988\n",
      "  Cobertura: 62.4%\n"
     ]
    }
   ],
   "source": [
    "## 9.1 Cobertura de Vocabulario en TEST\n",
    "\n",
    "# T√©rminos √∫nicos en test\n",
    "test_vocab = set()\n",
    "for text in df_test['text_clean'].fillna(''):\n",
    "    test_vocab.update(text.split())\n",
    "\n",
    "# T√©rminos del vectorizador presentes en test\n",
    "vocab_terms = set(tfidf_vectorizer.vocabulary_.keys())\n",
    "overlap = test_vocab.intersection(vocab_terms)\n",
    "\n",
    "print(f\"\\nüìä COBERTURA DE VOCABULARIO:\")\n",
    "print(f\"  T√©rminos √∫nicos en TEST: {len(test_vocab)}\")\n",
    "print(f\"  T√©rminos en vocabulario TF-IDF: {len(vocab_terms)}\")\n",
    "print(f\"  T√©rminos en com√∫n: {len(overlap)}\")\n",
    "print(f\"  Cobertura: {len(overlap)/len(test_vocab)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed57d4e",
   "metadata": {},
   "source": [
    "## 12. Resumen Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bc7fa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RESUMEN DE VECTORIZACI√ìN\n",
      "============================================================\n",
      "\n",
      "üìä DATOS FINALES:\n",
      "  Train: 1,596,781 tweets √ó 10,007 features\n",
      "  Test:  359 tweets √ó 10,007 features\n",
      "\n",
      "üî§ COMPOSICI√ìN DE FEATURES:\n",
      "  - Vocabulario TF-IDF: 10,000 t√©rminos\n",
      "    * Unigramas: 6,121\n",
      "    * Bigramas: 3,879\n",
      "  - Features num√©ricas: 7\n",
      "  - TOTAL: 10,007 features\n",
      "\n",
      "‚úÖ DECISIONES DEL EDA IMPLEMENTADAS:\n",
      "  ‚úì TF-IDF con bigramas ('not good', 'very happy')\n",
      "  ‚úì Stopwords personalizadas (conservamos 'not', 'no', 'very')\n",
      "  ‚úì 7 features num√©ricas integradas\n",
      "  ‚úì Vocabulario limitado a 10,000 features\n",
      "  ‚úì Matriz sparse eficiente (sparsity: 99.89%)\n",
      "\n",
      "üéØ LISTO PARA MODELADO\n",
      "  Pr√≥ximo paso: Notebook 4 - Entrenar modelos\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"RESUMEN DE VECTORIZACI√ìN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä DATOS FINALES:\")\n",
    "print(f\"  Train: {X_train_final.shape[0]:,} tweets √ó {X_train_final.shape[1]:,} features\")\n",
    "print(f\"  Test:  {X_test_final.shape[0]:,} tweets √ó {X_test_final.shape[1]:,} features\")\n",
    "\n",
    "print(f\"\\nüî§ COMPOSICI√ìN DE FEATURES:\")\n",
    "print(f\"  - Vocabulario TF-IDF: {len(tfidf_vectorizer.vocabulary_):,} t√©rminos\")\n",
    "print(f\"    * Unigramas: {len(unigramas):,}\")\n",
    "print(f\"    * Bigramas: {len(bigramas):,}\")\n",
    "print(f\"  - Features num√©ricas: 7\")\n",
    "print(f\"  - TOTAL: {X_train_final.shape[1]:,} features\")\n",
    "\n",
    "print(f\"\\n‚úÖ DECISIONES DEL EDA IMPLEMENTADAS:\")\n",
    "print(f\"  ‚úì TF-IDF con bigramas ('not good', 'very happy')\")\n",
    "print(f\"  ‚úì Stopwords personalizadas (conservamos 'not', 'no', 'very')\")\n",
    "print(f\"  ‚úì 7 features num√©ricas integradas\")\n",
    "print(f\"  ‚úì Vocabulario limitado a {X_train_tfidf.shape[1]:,} features\")\n",
    "print(f\"  ‚úì Matriz sparse eficiente (sparsity: {(1 - X_train_final.nnz / (X_train_final.shape[0] * X_train_final.shape[1])) * 100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nüéØ LISTO PARA MODELADO\")\n",
    "print(f\"  Pr√≥ximo paso: Notebook 4 - Entrenar modelos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3816339",
   "metadata": {},
   "source": [
    "## 13. Word2Vec Embeddings\n",
    "\n",
    "Word2Vec es otra t√©cnica de vectorizaci√≥n que genera embeddings densos (vectores de 100-300 dimensiones) donde palabras similares quedan cerca en el espacio vectorial.\n",
    "\n",
    "A diferencia de TF-IDF que usa frecuencias, Word2Vec aprende representaciones sem√°nticas del contexto en que aparecen las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c973e6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# ENTRENAMIENTO DEL MODELO WORD2VEC (ejecutar solo si es necesario)\n",
    "# =============================================================\n",
    "# El modelo ya est√° entrenado y guardado en models/word2vec_model.pkl\n",
    "# Descomentar solo si necesit√°s re-entrenar.\n",
    "\n",
    "# from gensim.models import Word2Vec\n",
    "# import pandas as pd\n",
    "#\n",
    "# # Cargar datos preprocesados\n",
    "# df_train = pd.read_csv('../data/processed/train_processed.csv')\n",
    "# sentences = [text.split() for text in df_train['text_clean'].fillna('').tolist()]\n",
    "#\n",
    "# # Entrenar Word2Vec\n",
    "# model_w2v = Word2Vec(\n",
    "#     sentences=sentences,\n",
    "#     vector_size=100,\n",
    "#     window=5,\n",
    "#     min_count=5,\n",
    "#     workers=4,\n",
    "#     epochs=10\n",
    "# )\n",
    "#\n",
    "# # Guardar modelo\n",
    "# model_w2v.save('../models/word2vec_model.pkl')\n",
    "# print(f\"‚úì Modelo guardado con {len(model_w2v.wv):,} palabras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e412be18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo Word2Vec cargado: 57,795 palabras\n",
      "   Vector size: 100 dimensiones\n",
      "\n",
      "üìä Ejemplo - Palabras similares a 'happy':\n",
      "   happyy: 0.6584\n",
      "   thrilled: 0.6207\n",
      "   pleased: 0.6005\n",
      "   sad: 0.5942\n",
      "   upset: 0.5836\n"
     ]
    }
   ],
   "source": [
    "# Verificar modelo guardado\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model_w2v = Word2Vec.load('../models/word2vec_model.pkl')\n",
    "print(f\"‚úÖ Modelo Word2Vec cargado: {len(model_w2v.wv):,} palabras\")\n",
    "print(f\"   Vector size: {model_w2v.wv.vector_size} dimensiones\")\n",
    "\n",
    "# Ejemplo de palabras similares\n",
    "print(f\"\\nüìä Ejemplo - Palabras similares a 'happy':\")\n",
    "for word, score in model_w2v.wv.most_similar('happy', topn=5):\n",
    "    print(f\"   {word}: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
