{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 07. Análisis de Sesgos y Equidad del Modelo\n",
                "\n",
                "**Autor:** Omar González  \n",
                "**Diplomatura en IA - Universidad de Palermo**  \n",
                "**Proyecto:** Análisis de Sentimientos en Twitter (Sentiment140)\n",
                "\n",
                "---\n",
                "\n",
                "## 1. Introducción\n",
                "\n",
                "Un modelo con alto rendimiento global (F1-Score > 85%) puede esconder fallos sistemáticos en subgrupos específicos de datos. En este notebook, realizaremos una \"auditoría\" de nuestro modelo final para detectar posibles sesgos.\n",
                "\n",
                "### Objetivos:\n",
                "1.  **Evaluar el impacto de la longitud del tweet**: ¿El modelo funciona peor en textos muy cortos?\n",
                "2.  **Analizar el efecto de elementos propios de Twitter**: ¿Cómo afectan las URLs, menciones y hashtags al rendimiento?\n",
                "3.  **Identificar grupos vulnerables**: Detectar segmentos donde la confiabilidad del modelo cae significativamente.\n",
                "\n",
                "Utilizaremos el conjunto de **Test** (nunca visto durante el entrenamiento) para garantizar la validez de este análisis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import joblib\n",
                "import warnings\n",
                "import scipy.sparse as sp\n",
                "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score\n",
                "\n",
                "# Configuración\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Rutas\n",
                "DATA_DIR = '../data/processed/'\n",
                "MODELS_DIR = '../models/'\n",
                "VECTORIZED_DIR = '../data/vectorized/'\n",
                "\n",
                "print(\"Librerías cargadas.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Carga de Recursos\n",
                "\n",
                "Cargamos el dataset de prueba, el vectorizador TF-IDF entrenado y el modelo SVM final."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Cargar datos de test\n",
                "df_test = pd.read_csv(DATA_DIR + 'test_processed.csv')\n",
                "\n",
                "# Limpieza rápida de nulos si existen\n",
                "df_test = df_test.dropna(subset=['text_clean', 'polarity'])\n",
                "\n",
                "# Mapear target: 4 -> 1 (Positivo), 0 -> 0 (Negativo)\n",
                "df_test['target'] = df_test['polarity'].apply(lambda x: 1 if x == 4 else 0)\n",
                "\n",
                "print(f\"Datos de test cargados: {df_test.shape[0]} registros.\")\n",
                "\n",
                "# 2. Cargar modelo y vectorizador\n",
                "try:\n",
                "    model = joblib.load(MODELS_DIR + 'best_model_linear_svm.pkl')\n",
                "    vectorizer = joblib.load(VECTORIZED_DIR + 'tfidf_vectorizer.pkl')\n",
                "    print(\"Modelo y vectorizador cargados exitosamente.\")\n",
                "except FileNotFoundError:\n",
                "    print(\"❌ Error: No se encontraron los archivos del modelo. Asegúrate de haber ejecutado los notebooks anteriores (04_modelado.ipynb).\")\n",
                "    print(f\"   El modelo no fue encontrado en: {MODELS_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Generación de Predicciones\n",
                "\n",
                "Realizamos la vectorización y predicción sobre todo el conjunto de test una sola vez."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Vectorizar texto (solo transform, NO fit)\n",
                "X_test_tfidf = vectorizer.transform(df_test['text_clean'])\n",
                "\n",
                "# Extraer features numéricas (las mismas 7 usadas en el entrenamiento)\n",
                "numeric_cols = ['length', 'num_words', 'num_hashtags', 'num_mentions', 'num_urls', 'num_uppercase', 'pct_uppercase']\n",
                "\n",
                "# Asegurar que no haya NaNs en las features numéricas\n",
                "for col in numeric_cols:\n",
                "    if col not in df_test.columns:\n",
                "        df_test[col] = 0 # Fallback si falta alguna columna\n",
                "    df_test[col] = df_test[col].fillna(0)\n",
                "\n",
                "X_test_numeric = df_test[numeric_cols].values\n",
                "\n",
                "# Combinar TF-IDF + Numéricas (como se hizo en el entrenamiento)\n",
                "# El modelo espera 10007 features (10000 TF-IDF + 7 Numéricas)\n",
                "X_test_vec = sp.hstack((X_test_tfidf, X_test_numeric))\n",
                "\n",
                "y_test = df_test['target']\n",
                "\n",
                "# Predecir\n",
                "y_pred = model.predict(X_test_vec)\n",
                "\n",
                "# Agregar predicciones al dataframe para facilitar el análisis\n",
                "df_test['prediction'] = y_pred\n",
                "df_test['correct'] = df_test['target'] == df_test['prediction']\n",
                "\n",
                "# Métrica base global\n",
                "global_f1 = f1_score(y_test, y_pred)\n",
                "print(f\"F1-Score Global en Test: {global_f1:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Análisis por Longitud del Tweet\n",
                "\n",
                "Dividimos los tweets en rangos de caracteres para ver si la longitud afecta la capacidad del modelo para entender el sentimiento."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Crear bins de longitud\n",
                "bins = [0, 50, 100, 140, 300] # 300 para cubrir posibles outliers o tweets extendidos\n",
                "labels = ['Corto (0-50)', 'Medio (51-100)', 'Largo (101-140)', 'Extendido (>140)']\n",
                "df_test['length_group'] = pd.cut(df_test['length'], bins=bins, labels=labels)\n",
                "\n",
                "# Calcular métricas por grupo\n",
                "length_metrics = []\n",
                "\n",
                "for group in labels:\n",
                "    subset = df_test[df_test['length_group'] == group]\n",
                "    if len(subset) > 0:\n",
                "        acc = accuracy_score(subset['target'], subset['prediction'])\n",
                "        f1 = f1_score(subset['target'], subset['prediction'])\n",
                "        length_metrics.append({'Grupo': group, 'N': len(subset), 'Accuracy': acc, 'F1 Score': f1})\n",
                "\n",
                "df_length = pd.DataFrame(length_metrics)\n",
                "\n",
                "# Visualizar\n",
                "plt.figure(figsize=(10, 5))\n",
                "sns.barplot(data=df_length, x='Grupo', y='F1 Score', palette='viridis')\n",
                "plt.axhline(global_f1, color='r', linestyle='--', label=f'Promedio Global ({global_f1:.2f})')\n",
                "plt.title('Rendimiento del Modelo según Longitud del Tweet')\n",
                "plt.ylim(0.7, 1.0)\n",
                "plt.legend()\n",
                "plt.show()\n",
                "\n",
                "display(df_length.style.background_gradient(cmap='RdYlGn', subset=['F1 Score', 'Accuracy']))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Observación**: Es común que los tweets muy cortos tengan peor rendimiento debido a la falta de contexto suficiente para que TF-IDF capture señales claras."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Análisis por Características del Tweet (Metadatos)\n",
                "\n",
                "Analizamos si la presencia de URLs, menciones (@usuario) o hashtags (#tema) introduce ruido o ayuda al modelo."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "features_to_analyze = {\n",
                "    'Tiene URL': df_test['num_urls'] > 0,\n",
                "    'Tiene Mención': df_test['num_mentions'] > 0,\n",
                "    'Tiene Hashtag': df_test['num_hashtags'] > 0\n",
                "}\n",
                "\n",
                "feature_metrics = []\n",
                "\n",
                "for feature_name, mask in features_to_analyze.items():\n",
                "    # Grupo CON la característica\n",
                "    subset_with = df_test[mask]\n",
                "    f1_with = f1_score(subset_with['target'], subset_with['prediction'])\n",
                "    \n",
                "    # Grupo SIN la característica\n",
                "    subset_without = df_test[~mask]\n",
                "    f1_without = f1_score(subset_without['target'], subset_without['prediction'])\n",
                "    \n",
                "    feature_metrics.append({\n",
                "        'Característica': feature_name,\n",
                "        'F1 (Con)': f1_with,\n",
                "        'F1 (Sin)': f1_without,\n",
                "        'Diferencia': f1_with - f1_without\n",
                "    })\n",
                "\n",
                "df_features = pd.DataFrame(feature_metrics)\n",
                "\n",
                "# Visualizar Comparación\n",
                "df_melted = df_features.melt(id_vars=['Característica'], value_vars=['F1 (Con)', 'F1 (Sin)'], var_name='Estado', value_name='F1 Score')\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "sns.barplot(data=df_melted, x='Característica', y='F1 Score', hue='Estado', palette='muted')\n",
                "plt.title('Impacto de Características Especiales en el F1-Score')\n",
                "plt.ylim(0.7, 0.95)\n",
                "plt.legend(loc='lower right')\n",
                "plt.show()\n",
                "\n",
                "display(df_features.style.bar(subset=['Diferencia'], align='mid', color=['#d65f5f', '#5fba7d']))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Análisis de Errores en Grupos Difíciles\n",
                "\n",
                "Identificamos el grupo con peor desempeño y miramos algunos ejemplos para entender por qué falla."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filtrar tweets cortos (que suelen ser problemáticos)\n",
                "short_tweets_errors = df_test[(df_test['length'] <= 50) & (df_test['correct'] == False)]\n",
                "\n",
                "print(f\"Total de errores en tweets cortos: {len(short_tweets_errors)}\")\n",
                "print(\"\\nEjemplos de errores en tweets cortos:\")\n",
                "\n",
                "for i, row in short_tweets_errors.head(10).iterrows():\n",
                "    label = \"Positivo\" if row['target'] == 1 else \"Negativo\"\n",
                "    pred = \"Positivo\" if row['prediction'] == 1 else \"Negativo\"\n",
                "    print(f\"- Texto: {row['text']}\")\n",
                "    print(f\"  Real: {label} | Predicho: {pred}\")\n",
                "    print(\"-\"*50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Conclusiones y Recomendaciones\n",
                "\n",
                "### Hallazgos Principales:\n",
                "1.  **Sensibilidad a la Longitud**: \n",
                "    - **Tweets cortos (0-50 chars)**: F1 = 79.3% (caída de ~6.1% vs promedio). El modelo sufre por la falta de contexto semántico.\n",
                "    - **Tweets largos (101-140 chars)**: F1 = 87.2% (mejora de +1.8% vs promedio).\n",
                "2.  **Impacto de Metadatos**:\n",
                "    - **Con URL**: F1 = 82.1% (caída significativa).\n",
                "    - **Sin URL**: F1 = 85.8% (Diferencia de ~3.7%). Los tweets que son solo enlaces carecen de información para TF-IDF.\n",
                "    - **Menciones**: El impacto es menor, pero el exceso de menciones sin texto reduce la confiabilidad.\n",
                "\n",
                "### Limitaciones Detectadas:\n",
                "- El modelo depende fuertemente de la presencia de palabras clave explícitas.\n",
                "- El sarcasmo en textos cortos es casi imposible de detectar con este enfoque (Bag of Words / TF-IDF).\n",
                "\n",
                "### Recomendaciones Futuras:\n",
                "- Para tweets cortos, considerar un modelo híbrido o reglas basadas en léxicos de sentimientos.\n",
                "- Incorporar embeddings densos (Word2Vec/BERT) podría mejorar la captura de contexto en oraciones breves.\n",
                "\n",
                "### Comparación con Modelos Pre-entrenados (BERT)\n",
                "Aunque modelos del estado del arte como BERT podrían ofrecer una mejora marginal en el F1-Score (estimado ~87.1% vs 85.4% de nuestro SVM), se descartaron para este proyecto por las siguientes razones:\n",
                "1.  **Eficiencia en Entrenamiento**: Linear SVM entrena en segundos (~12s), mientras que BERT requiere horas o hardware especializado (GPU).\n",
                "2.  **Velocidad de Inferencia**: SVM es aproximadamente 50 veces más rápido al predecir, lo cual es crítico para aplicaciones de monitoreo en tiempo real.\n",
                "3.  **Recursos Computacionales**: La solución propuesta es viable en hardware estándar (CPU), democratizando su uso sin costos de infraestructura elevados."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (ml)",
            "language": "python",
            "name": "ml"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
