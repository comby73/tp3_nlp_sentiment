{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ed8129b",
   "metadata": {},
   "source": [
    "## 1. Configuraci√≥n e Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b235df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Librer√≠as y m√≥dulos cargados\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Agregar src al path\n",
    "project_dir = Path.cwd().parent\n",
    "if str(project_dir) not in sys.path:\n",
    "    sys.path.append(str(project_dir))\n",
    "\n",
    "# Agregar src al path expl√≠citamente para imports internos\n",
    "src_dir = project_dir / 'src'\n",
    "if str(src_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(src_dir))\n",
    "\n",
    "from config import RANDOM_SEED\n",
    "from src.data_loading import load_raw_training_data, load_raw_test_data, save_processed_data\n",
    "from src.preprocessing import (\n",
    "    clean_text, \n",
    "    extract_urls, \n",
    "    extract_mentions, \n",
    "    extract_hashtags,\n",
    "    detect_intensified_words,\n",
    "    calculate_uppercase_ratio,\n",
    "    preprocess_text\n",
    ")\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(\"‚úì Librer√≠as y m√≥dulos cargados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525f9452",
   "metadata": {},
   "source": [
    "## 2. Cargar Datos Crudos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "269ce5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dataset de entrenamiento cargado: 1600000 filas\n",
      "‚úì Dataset de test cargado: 498 filas\n",
      "Train: (1600000, 6)\n",
      "Test: (498, 6)\n",
      "\n",
      "Clases en train: [0 4]\n",
      "Clases en test: [4 0 2]\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos usando src\n",
    "# nrows=None para cargar todo, nrows=100000 para pruebas\n",
    "df_train = load_raw_training_data(nrows=None)\n",
    "df_test = load_raw_test_data()\n",
    "\n",
    "print(f\"Train: {df_train.shape}\")\n",
    "print(f\"Test: {df_test.shape}\")\n",
    "print(f\"\\nClases en train: {df_train['polarity'].unique()}\")\n",
    "print(f\"Clases en test: {df_test['polarity'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "leakage_fix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VERIFICACI√ìN Y LIMPIEZA DE DATA LEAKAGE ===\n",
      "\n",
      "üìä IDs √∫nicos en train: 1,598,315\n",
      "üìä IDs √∫nicos en test (original): 498\n",
      "\n",
      "üîç IDs duplicados entre train y test: 0\n",
      "\n",
      "‚úÖ No hay data leakage\n",
      "\n",
      "üìä Shapes finales:\n",
      "   Train: (1600000, 6)\n",
      "   Test: (498, 6)\n"
     ]
    }
   ],
   "source": [
    "## 2.1. ELIMINAR DATA LEAKAGE: Remover tweets de test que est√°n en train\n",
    "\n",
    "print(\"=== VERIFICACI√ìN Y LIMPIEZA DE DATA LEAKAGE ===\\n\")\n",
    "\n",
    "# Obtener IDs √∫nicos de train\n",
    "train_ids = set(df_train['id'].unique())\n",
    "print(f\"üìä IDs √∫nicos en train: {len(train_ids):,}\")\n",
    "\n",
    "# Obtener IDs √∫nicos de test\n",
    "test_ids_original = set(df_test['id'].unique())\n",
    "print(f\"üìä IDs √∫nicos en test (original): {len(test_ids_original):,}\")\n",
    "\n",
    "# Detectar duplicados\n",
    "overlap_ids = train_ids.intersection(test_ids_original)\n",
    "print(f\"\\nüîç IDs duplicados entre train y test: {len(overlap_ids):,}\")\n",
    "\n",
    "if len(overlap_ids) > 0:\n",
    "    # Eliminar tweets de test que est√°n en train\n",
    "    df_test = df_test[~df_test['id'].isin(train_ids)].reset_index(drop=True)\n",
    "    print(f\"\\n‚úÖ Data leakage eliminado:\")\n",
    "    print(f\"   - Tweets removidos: {len(overlap_ids):,}\")\n",
    "    print(f\"   - Test despu√©s de limpieza: {len(df_test):,}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No hay data leakage\")\n",
    "\n",
    "print(f\"\\nüìä Shapes finales:\")\n",
    "print(f\"   Train: {df_train.shape}\")\n",
    "print(f\"   Test: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4291230f",
   "metadata": {},
   "source": [
    "## 3. Implementar Funci√≥n de Preprocesamiento\n",
    "\n",
    "Esta funci√≥n implementa las 9 decisiones del EDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a3bd3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: @user This is SOOO COOL!!! #awesome http://example.com\n",
      "Procesado: this is sooo cool awesome\n"
     ]
    }
   ],
   "source": [
    "# Usaremos la funci√≥n clean_text del m√≥dulo src.preprocessing\n",
    "# Esta funci√≥n implementa la limpieza est√°ndar definida en el proyecto.\n",
    "\n",
    "def preprocess_tweet(text):\n",
    "    return clean_text(text)\n",
    "\n",
    "# Test\n",
    "test_text = \"@user This is SOOO COOL!!! #awesome http://example.com\"\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Procesado: {preprocess_tweet(test_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978d7c06",
   "metadata": {},
   "source": [
    "## 4. Crear Features Num√©ricas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3ab309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'length': 54, 'num_words': 7, 'num_hashtags': 1, 'num_mentions': 1, 'num_urls': 1, 'num_uppercase': 9, 'pct_uppercase': 23.076923076923077, 'num_intensified': 1}\n"
     ]
    }
   ],
   "source": [
    "def extract_features(text):\n",
    "    \"\"\"\n",
    "    Extrae features num√©ricas usando funciones auxiliares de src.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return {\n",
    "            'length': 0, 'num_words': 0, 'num_hashtags': 0, \n",
    "            'num_mentions': 0, 'num_urls': 0, 'num_uppercase': 0, 'pct_uppercase': 0.0,\n",
    "            'num_intensified': 0\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'length': len(text),\n",
    "        'num_words': len(text.split()),\n",
    "        'num_hashtags': len(extract_hashtags(text)),\n",
    "        'num_mentions': len(extract_mentions(text)),\n",
    "        'num_urls': len(extract_urls(text)),\n",
    "        'num_uppercase': sum(1 for c in text if c.isupper()),\n",
    "        'pct_uppercase': calculate_uppercase_ratio(text) * 100,\n",
    "        'num_intensified': detect_intensified_words(text)  \n",
    "    }\n",
    "\n",
    "# Test\n",
    "test_text = \"@user This is SOOO COOL!!! #awesome http://example.com\"\n",
    "print(extract_features(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629a053f",
   "metadata": {},
   "source": [
    "## 5. Aplicar Preprocesamiento a TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23abf802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando dataset TRAIN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1600000/1600000 [00:23<00:00, 67483.50it/s]\n",
      "Limpiando texto: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1600000/1600000 [00:18<00:00, 87667.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Train procesado: (1596781, 11)\n",
      "Columnas: ['polarity', 'text', 'text_clean', 'length', 'num_words', 'num_hashtags', 'num_mentions', 'num_urls', 'num_uppercase', 'pct_uppercase', 'num_intensified']\n",
      "\n",
      "Ejemplo:\n",
      "                                                text  \\\n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
      "1  is upset that he can't update his Facebook by ...   \n",
      "2  @Kenichan I dived many times for the ball. Man...   \n",
      "\n",
      "                                          text_clean  length  num_hashtags  \n",
      "0  a thats a bummer you shoulda got david carr of...     115             0  \n",
      "1  is upset that he cant update his facebook by t...     111             0  \n",
      "2  i dived many times for the ball managed to sav...      89             0  \n"
     ]
    }
   ],
   "source": [
    "print(\"Procesando dataset TRAIN...\")\n",
    "\n",
    "# Extraer features num√©ricas ANTES de limpiar\n",
    "tqdm.pandas(desc=\"Extrayendo features\")\n",
    "features_train = df_train['text'].progress_apply(extract_features)\n",
    "features_train_df = pd.DataFrame(features_train.tolist())\n",
    "\n",
    "# Limpiar texto\n",
    "tqdm.pandas(desc=\"Limpiando texto\")\n",
    "df_train['text_clean'] = df_train['text'].progress_apply(preprocess_tweet)\n",
    "\n",
    "# Combinar todo\n",
    "df_train_processed = pd.concat([\n",
    "    df_train[['polarity', 'text', 'text_clean']],\n",
    "    features_train_df\n",
    "], axis=1)\n",
    "\n",
    "# Eliminar textos vac√≠os despu√©s de limpieza\n",
    "df_train_processed = df_train_processed[df_train_processed['text_clean'].str.len() > 0]\n",
    "\n",
    "print(f\"\\n‚úì Train procesado: {df_train_processed.shape}\")\n",
    "print(f\"Columnas: {list(df_train_processed.columns)}\")\n",
    "print(f\"\\nEjemplo:\")\n",
    "print(df_train_processed[['text', 'text_clean', 'length', 'num_hashtags']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ddd8f1",
   "metadata": {},
   "source": [
    "## 6. Aplicar Preprocesamiento a TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3f31a9",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Decisi√≥n Cr√≠tica: Eliminaci√≥n de Clase Neutral\n",
    "\n",
    "**Contexto:**\n",
    "- El dataset de **training NO contiene clase neutral** (solo 0=negativo y 4=positivo)\n",
    "- El dataset de **test S√ç contiene 139 tweets neutrales** (clase 2), que representan el 28% del test\n",
    "\n",
    "**Justificaci√≥n para eliminar neutrales del test:**\n",
    "\n",
    "1. **Consistencia metodol√≥gica**: Un modelo entrenado solo con clases [0, 4] no puede aprender a predecir la clase [2] que nunca vio.\n",
    "\n",
    "2. **Imposibilidad matem√°tica**: Pedirle al modelo que clasifique neutrales ser√≠a como entrenar para distinguir perros/gatos y luego evaluar con p√°jaros.\n",
    "\n",
    "3. **Evaluaci√≥n honesta**: Al evaluar solo 0 vs 4, medimos la verdadera capacidad del modelo en la tarea binaria para la que fue entrenado.\n",
    "\n",
    "4. **Subjetividad de neutralidad**: La clase neutral es altamente subjetiva en el etiquetado manual (ej: \"Going to the store\" puede ser neutral o positivo seg√∫n contexto).\n",
    "\n",
    "5. **Aplicaci√≥n real**: Muchos sistemas de an√°lisis de sentimiento en producci√≥n son binarios (positivo/negativo).\n",
    "\n",
    "**Consecuencia:**\n",
    "- **Test original**: 498 tweets (182 pos, 139 neutral, 177 neg)\n",
    "- **Test efectivo**: 359 tweets (182 pos, 177 neg)\n",
    "- **Balance final**: 50.7% positivos / 49.3% negativos ‚úì (casi perfecto)\n",
    "\n",
    "**M√©tricas a evaluar** (solo sobre clases 0 y 4):\n",
    "- Accuracy, Precision, Recall, F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aa70a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando dataset TEST...\n",
      "Test despu√©s de remover neutrales: (359, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 359/359 [00:00<00:00, 52447.06it/s]\n",
      "Limpiando texto: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 359/359 [00:00<00:00, 69172.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Test procesado: (359, 11)\n",
      "Clases finales: polarity\n",
      "4.0    182\n",
      "0.0    177\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Procesando dataset TEST...\")\n",
    "\n",
    "# DECISI√ìN 9: Remover clase neutral (2) del test\n",
    "df_test = df_test[df_test['polarity'].isin([0, 4])]\n",
    "print(f\"Test despu√©s de remover neutrales: {df_test.shape}\")\n",
    "\n",
    "# Extraer features num√©ricas\n",
    "tqdm.pandas(desc=\"Extrayendo features\")\n",
    "features_test = df_test['text'].progress_apply(extract_features)\n",
    "features_test_df = pd.DataFrame(features_test.tolist())\n",
    "\n",
    "# Limpiar texto\n",
    "tqdm.pandas(desc=\"Limpiando texto\")\n",
    "df_test['text_clean'] = df_test['text'].progress_apply(preprocess_tweet)\n",
    "\n",
    "# Combinar todo\n",
    "df_test_processed = pd.concat([\n",
    "    df_test[['polarity', 'text', 'text_clean']],\n",
    "    features_test_df\n",
    "], axis=1)\n",
    "\n",
    "# Eliminar textos vac√≠os\n",
    "df_test_processed = df_test_processed[df_test_processed['text_clean'].str.len() > 0]\n",
    "\n",
    "print(f\"\\n‚úì Test procesado: {df_test_processed.shape}\")\n",
    "print(f\"Clases finales: {df_test_processed['polarity'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a43e1dd",
   "metadata": {},
   "source": [
    "## 7. Verificar Calidad del Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0797ac1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPARACI√ìN ANTES/DESPU√âS ===\n",
      "\n",
      "Ejemplos TRAIN:\n",
      "\n",
      "Original: @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got D\n",
      "Limpio:   a thats a bummer you shoulda got david carr of third day to do it d\n",
      "Features: length=115, hashtags=0, mentions=1\n",
      "\n",
      "Original: is upset that he can't update his Facebook by texting it... and might cry as a r\n",
      "Limpio:   is upset that he cant update his facebook by texting it and might cry as a resul\n",
      "Features: length=111, hashtags=0, mentions=0\n",
      "\n",
      "Original: @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out \n",
      "Limpio:   i dived many times for the ball managed to save the rest go out of bounds\n",
      "Features: length=89, hashtags=0, mentions=1\n",
      "\n",
      "Original: my whole body feels itchy and like its on fire \n",
      "Limpio:   my whole body feels itchy and like its on fire\n",
      "Features: length=47, hashtags=0, mentions=0\n",
      "\n",
      "Original: @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I\n",
      "Limpio:   no its not behaving at all im mad why am i here because i cant see you all over \n",
      "Features: length=111, hashtags=0, mentions=1\n"
     ]
    }
   ],
   "source": [
    "# Comparar antes/despu√©s\n",
    "print(\"=== COMPARACI√ìN ANTES/DESPU√âS ===\")\n",
    "print(\"\\nEjemplos TRAIN:\")\n",
    "for i in range(5):\n",
    "    print(f\"\\nOriginal: {df_train_processed.iloc[i]['text'][:80]}\")\n",
    "    print(f\"Limpio:   {df_train_processed.iloc[i]['text_clean'][:80]}\")\n",
    "    print(f\"Features: length={df_train_processed.iloc[i]['length']}, \"\n",
    "          f\"hashtags={df_train_processed.iloc[i]['num_hashtags']}, \"\n",
    "          f\"mentions={df_train_processed.iloc[i]['num_mentions']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c139608b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ESTAD√çSTICAS DE LIMPIEZA ===\n",
      "\n",
      "TRAIN:\n",
      "  - Longitud promedio original: 74.2\n",
      "  - Longitud promedio limpia: 62.1\n",
      "  - Tweets con hashtags: 35846\n",
      "  - Tweets con mentions: 735297\n",
      "  - Tweets con URLs: 80960\n",
      "\n",
      "TEST:\n",
      "  - Longitud promedio original: 84.3\n",
      "  - Longitud promedio limpia: 71.2\n",
      "  - Tweets con hashtags: 17\n",
      "  - Tweets con mentions: 64\n",
      "  - Tweets con URLs: 39\n"
     ]
    }
   ],
   "source": [
    "# Estad√≠sticas de limpieza\n",
    "print(\"\\n=== ESTAD√çSTICAS DE LIMPIEZA ===\")\n",
    "print(f\"\\nTRAIN:\")\n",
    "print(f\"  - Longitud promedio original: {df_train_processed['length'].mean():.1f}\")\n",
    "print(f\"  - Longitud promedio limpia: {df_train_processed['text_clean'].str.len().mean():.1f}\")\n",
    "print(f\"  - Tweets con hashtags: {(df_train_processed['num_hashtags'] > 0).sum()}\")\n",
    "print(f\"  - Tweets con mentions: {(df_train_processed['num_mentions'] > 0).sum()}\")\n",
    "print(f\"  - Tweets con URLs: {(df_train_processed['num_urls'] > 0).sum()}\")\n",
    "\n",
    "print(f\"\\nTEST:\")\n",
    "print(f\"  - Longitud promedio original: {df_test_processed['length'].mean():.1f}\")\n",
    "print(f\"  - Longitud promedio limpia: {df_test_processed['text_clean'].str.len().mean():.1f}\")\n",
    "print(f\"  - Tweets con hashtags: {(df_test_processed['num_hashtags'] > 0).sum()}\")\n",
    "print(f\"  - Tweets con mentions: {(df_test_processed['num_mentions'] > 0).sum()}\")\n",
    "print(f\"  - Tweets con URLs: {(df_test_processed['num_urls'] > 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef97f54",
   "metadata": {},
   "source": [
    "## 8. Guardar Datos Procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "932c5521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Datos guardados en: d:\\Diplomatura en ia\\trabajo practico 3 -Omar Gonzalez\\tp3_nlp_sentiment\\data\\processed\\train_processed.csv\n",
      "‚úì Datos guardados en: d:\\Diplomatura en ia\\trabajo practico 3 -Omar Gonzalez\\tp3_nlp_sentiment\\data\\processed\\test_processed.csv\n",
      "\n",
      "TRAIN: (1596781, 11)\n",
      "TEST: (359, 11)\n"
     ]
    }
   ],
   "source": [
    "# Guardar usando src\n",
    "save_processed_data(df_train_processed, 'train_processed.csv')\n",
    "save_processed_data(df_test_processed, 'test_processed.csv')\n",
    "\n",
    "print(f\"\\nTRAIN: {df_train_processed.shape}\")\n",
    "print(f\"TEST: {df_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "validation_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VALIDACI√ìN DE COHERENCIA ===\n",
      "\n",
      "‚úÖ Integridad de datos:\n",
      "   Train procesado: 1,596,781 tweets\n",
      "   Test procesado: 359 tweets\n",
      "\n",
      "‚úÖ Distribuci√≥n TRAIN:\n",
      "polarity\n",
      "4    798398\n",
      "0    798383\n",
      "Name: count, dtype: int64\n",
      "   Balance: 50.0% neg\n",
      "\n",
      "‚úÖ Distribuci√≥n TEST:\n",
      "polarity\n",
      "4.0    182\n",
      "0.0    177\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚úÖ No hay textos vac√≠os\n",
      "\n",
      "=== RANGO DE FEATURES NUM√âRICAS ===\n",
      "length               | Train: (np.int64(6), np.int64(374)) | Test: (np.float64(12.0), np.float64(144.0))\n",
      "num_words            | Train: (np.int64(1), np.int64(64)) | Test: (np.float64(2.0), np.float64(30.0))\n",
      "num_hashtags         | Train: (np.int64(0), np.int64(24)) | Test: (np.float64(0.0), np.float64(3.0))\n",
      "num_mentions         | Train: (np.int64(0), np.int64(12)) | Test: (np.float64(0.0), np.float64(4.0))\n",
      "num_urls             | Train: (np.int64(0), np.int64(5)) | Test: (np.float64(0.0), np.float64(1.0))\n",
      "num_uppercase        | Train: (np.int64(0), np.int64(131)) | Test: (np.float64(0.0), np.float64(108.0))\n",
      "pct_uppercase        | Train: (np.float64(0.0), np.float64(100.0)) | Test: (np.float64(0.0), np.float64(100.0))\n",
      "num_intensified      | Train: (np.int64(0), np.int64(9)) | Test: (np.float64(0.0), np.float64(2.0))\n",
      "\n",
      "‚úÖ Validaci√≥n completa. Listo para vectorizaci√≥n.\n"
     ]
    }
   ],
   "source": [
    "## 9. Validaci√≥n Final: Verificar Calidad de Datos Procesados\n",
    "\n",
    "print(\"=== VALIDACI√ìN DE COHERENCIA ===\\n\")\n",
    "\n",
    "# 1. Verificar integridad de procesamiento\n",
    "print(\"‚úÖ Integridad de datos:\")\n",
    "print(f\"   Train procesado: {len(df_train_processed):,} tweets\")\n",
    "print(f\"   Test procesado: {len(df_test_processed):,} tweets\")\n",
    "\n",
    "# 2. Verificar distribuci√≥n de clases\n",
    "print(f\"\\n‚úÖ Distribuci√≥n TRAIN:\")\n",
    "print(df_train_processed['polarity'].value_counts())\n",
    "print(f\"   Balance: {df_train_processed['polarity'].value_counts()[0] / len(df_train_processed) * 100:.1f}% neg\")\n",
    "\n",
    "print(f\"\\n‚úÖ Distribuci√≥n TEST:\")\n",
    "print(df_test_processed['polarity'].value_counts())\n",
    "\n",
    "# 3. Verificar que NO hay textos vac√≠os\n",
    "empty_train = (df_train_processed['text_clean'].str.len() == 0).sum()\n",
    "empty_test = (df_test_processed['text_clean'].str.len() == 0).sum()\n",
    "\n",
    "if empty_train > 0 or empty_test > 0:\n",
    "    print(f\"\\n‚ùå ERROR: Hay {empty_train} textos vac√≠os en TRAIN y {empty_test} en TEST\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No hay textos vac√≠os\")\n",
    "\n",
    "# 4. Verificar features num√©ricas (no pueden ser todas 0)\n",
    "numeric_cols = ['length', 'num_words', 'num_hashtags', 'num_mentions', \n",
    "                'num_urls', 'num_uppercase', 'pct_uppercase', 'num_intensified']\n",
    "\n",
    "print(\"\\n=== RANGO DE FEATURES NUM√âRICAS ===\")\n",
    "for col in numeric_cols:\n",
    "    train_range = (df_train_processed[col].min(), df_train_processed[col].max())\n",
    "    test_range = (df_test_processed[col].min(), df_test_processed[col].max())\n",
    "    print(f\"{col:20s} | Train: {train_range} | Test: {test_range}\")\n",
    "\n",
    "print(\"\\n‚úÖ Validaci√≥n completa. Listo para vectorizaci√≥n.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explanation_markdown",
   "metadata": {},
   "source": [
    "## 9. ¬øPor Qu√© Este Preprocesamiento?\n",
    "\n",
    "### El Problema: Texto ‚â† N√∫meros\n",
    "\n",
    "Los modelos de Machine Learning **no pueden procesar texto directamente**. Necesitan n√∫meros.\n",
    "\n",
    "**Ejemplo:**\n",
    "```\n",
    "Tweet original: \"@user I LOOOVE this!!! #awesome http://example.com\"\n",
    "                ‚Üì\n",
    "Tweet limpio:   \"i love this awesome\"\n",
    "                ‚Üì\n",
    "Vector TF-IDF:  [0.0, 0.52, 0.0, 0.61, ...]\n",
    "```\n",
    "\n",
    "**¬øPor qu√© limpiamos?**  \n",
    "Para que \"I LOVE this\" y \"i love this\" sean **la misma palabra** para el modelo.\n",
    "\n",
    "\n",
    "### Decisiones Basadas en el EDA\n",
    "\n",
    "**1. URLs eliminadas ‚Üí Feature creada**\n",
    "- **Raz√≥n**: Solo 5.1% de tweets tienen URLs y no aportan sentimiento\n",
    "- **Acci√≥n**: Eliminadas del texto + conteo en `num_urls`\n",
    "\n",
    "**2. Mentions eliminadas ‚Üí Feature creada**\n",
    "- **Raz√≥n**: 46.2% de tweets tienen mentions, pero son NOMBRES, no sentimiento\n",
    "- **Acci√≥n**: Eliminadas del texto + conteo en `num_mentions`\n",
    "\n",
    "**3. Hashtags convertidos a texto**\n",
    "- **Raz√≥n**: 2.2% de presencia pero son DISCRIMINATIVOS (#fail vs #awesome)\n",
    "- **Acci√≥n**: #awesome ‚Üí awesome (extraemos el texto)\n",
    "\n",
    "**4. May√∫sculas normalizadas**\n",
    "- **Raz√≥n**: UPPERCASE indica INTENSIDAD, no polaridad\n",
    "- **Acci√≥n**: Todo a lowercase + ratio de may√∫sculas guardado\n",
    "\n",
    "**5. Caracteres repetidos normalizados**\n",
    "- **Raz√≥n**: \"goooood\" y \"good\" deben ser LA MISMA palabra\n",
    "- **Acci√≥n**: Normalizaci√≥n de repeticiones excesivas\n",
    "\n",
    "**6. Clase neutral eliminada del test**\n",
    "- **Raz√≥n**: Es un problema BINARIO (positivo vs negativo)\n",
    "- **Acci√≥n**: Solo clases 0 y 4 conservadas\n",
    "\n",
    "### ¬øQu√© NO Hicimos (y Por Qu√©)?\n",
    "\n",
    "‚ùå **NO eliminamos duplicados**: Son expresiones naturales comunes (retweets)\n",
    "‚ùå **NO eliminamos stopwords a√∫n**: Se har√° en vectorizaci√≥n con TF-IDF\n",
    "‚ùå **NO lemmatizamos**: Se evaluar√° en el pr√≥ximo notebook\n",
    "‚ùå **NO usamos features temporales**: Dataset de 2009, no generalizar√≠a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1f9b6c",
   "metadata": {},
   "source": [
    "## 10. Resumen de lo Realizado\n",
    "\n",
    "### ‚úÖ Decisiones Implementadas:\n",
    "\n",
    "1. **URLs eliminadas**: Todos los http/www removidos\n",
    "2. **Mentions eliminadas**: Todos los @usuario removidos\n",
    "3. **Hashtags procesados**: #fail ‚Üí fail (texto extra√≠do)\n",
    "4. **May√∫sculas normalizadas**: Todo a lowercase + feature `num_uppercase` y `pct_uppercase`\n",
    "5. **Caracteres repetidos normalizados**: goooood ‚Üí good\n",
    "6. **8 Features num√©ricas extra√≠das**:\n",
    "   - `length`: longitud original\n",
    "   - `num_words`: cantidad de palabras\n",
    "   - `num_hashtags`: cantidad de hashtags\n",
    "   - `num_mentions`: cantidad de mentions\n",
    "   - `num_urls`: cantidad de URLs\n",
    "   - `num_uppercase`: cantidad de may√∫sculas\n",
    "   - `pct_uppercase`: porcentaje de may√∫sculas\n",
    "   - `num_intensified`: palabras con caracteres repetidos (goood, noooo)\n",
    "7. **Clase neutral removida del test**: Solo 0 (neg) y 4 (pos)\n",
    "8. **Textos vac√≠os eliminados**: Tweets sin contenido despu√©s de limpieza\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
